{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunt Game Notebook\n",
    "\n",
    "## Read and Review Your Starter Code\n",
    "The theme of this project is a popular treasure hunt game in which the player needs to find the treasure before the pirate does. While you will not be developing the entire game, you will write the part of the game that represents the intelligent agent, which is a pirate in this case. The pirate will try to find the optimal path to the treasure using deep Q-learning. \n",
    "\n",
    "You have been provided with two Python classes and this notebook to help you with this assignment. The first class, TreasureMaze.py, represents the environment, which includes a maze object defined as a matrix. The second class, GameExperience.py, stores the episodes â€“ that is, all the states that come in between the initial state and the terminal state. This is later used by the agent for learning by experience, called \"exploration\". This notebook shows how to play a game. Your task is to complete the deep Q-learning implementation for which a skeleton implementation has been provided. The code blocks you will need to complete has #TODO as a header.\n",
    "\n",
    "First, read and review the next few code and instruction blocks to understand the code that you have been given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from TreasureMaze import TreasureMaze\n",
    "from GameExperience import GameExperience\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block contains an 8x8 matrix that will be used as a maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function allows a visual representation of the maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    pirate_row, pirate_col, _ = qmaze.state\n",
    "    canvas[pirate_row, pirate_col] = 0.3   # pirate cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # treasure cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pirate agent can move in four directions: left, right, up, and down. \n",
    "\n",
    "While the agent primarily learns by experience through exploitation, often, the agent can choose to explore the environment to find previously undiscovered paths. This is called \"exploration\" and is defined by epsilon. This value is typically a lower value such as 0.1, which means for every ten attempts, the agent will attempt to learn by experience nine times and will randomly explore a new path one time. You are encouraged to try various values for the exploration factor and see how the algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code block and output below show creating a maze object and performing one action (DOWN), which returns the reward. The resulting updated environment is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f97cf6d08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = TreasureMaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simulates a full game based on the provided trained model. The other parameters include the TreasureMaze object and the starting position of the pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, pirate_cell):\n",
    "    qmaze.reset(pirate_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps you to determine whether the pirate can win any game at all. If your maze is not well designed, the pirate may not win any game at all. In this case, your training would not yield any result. The provided maze in this notebook ensures that there is a path to win and you can run this method to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you have been given in this block will build the neural network model. Review the code and note the number of layers, as well as the activation, optimizer, and loss functions that are used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #TODO: Complete the Q-Training Algorithm Code Block\n",
    "\n",
    "This is your deep Q-learning implementation. The goal of your deep Q-learning implementation is to find the best possible navigation sequence that results in reaching the treasure cell while maximizing the reward. In your implementation, you need to determine the optimal number of epochs to achieve a 100% win rate.\n",
    "\n",
    "You will need to complete the section starting with #pseudocode. The pseudocode has been included for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "\n",
    "    # exploration factor\n",
    "    global epsilon \n",
    "\n",
    "    # number of epochs\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "\n",
    "    # maximum memory to store episodes\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "\n",
    "    # maximum data size for training\n",
    "    data_size = opt.get('data_size', 50)\n",
    "\n",
    "    # start time\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = TreasureMaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "    \n",
    "    win_history = []   # history of win/lose game\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    \n",
    "    # ===============================\n",
    "    # Main training loop\n",
    "    # ===============================\n",
    "    for epoch in range(n_epoch):\n",
    "        # start new episode from a random free cell\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(agent_cell)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        loss = 0.0\n",
    "        n_episodes = 0\n",
    "        game_over = False\n",
    "\n",
    "        while not game_over:\n",
    "            prev_envstate = envstate\n",
    "\n",
    "            # epsilon-greedy choice\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(qmaze.valid_actions())\n",
    "            else:\n",
    "                q = model.predict(prev_envstate, verbose=0)[0]\n",
    "                # mask invalid\n",
    "                masked = np.full_like(q, -1e10, dtype=np.float32)\n",
    "                for a in qmaze.valid_actions():\n",
    "                    masked[a] = q[a]\n",
    "                action = int(np.argmax(masked))\n",
    "\n",
    "            # apply action\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            game_over = (game_status != 'not_over')\n",
    "\n",
    "            # store in memory\n",
    "            experience.remember([prev_envstate, action, reward, envstate, game_over])\n",
    "\n",
    "            # train from replay\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "            loss = h.history['loss'][0]\n",
    "\n",
    "            n_episodes += 1\n",
    "\n",
    "        # track win/loss\n",
    "        win = (game_status == 'win')\n",
    "        win_history.append(win)\n",
    "        win_rate = sum(win_history[-hsize:]) / hsize\n",
    "\n",
    "        # print progress every 50 epochs\n",
    "        if epoch % 50 == 0:\n",
    "            dt = datetime.datetime.now() - start_time\n",
    "            print(f\"Epoch {epoch:05d}/{n_epoch} | Loss {loss:.4f} | Episodes {n_episodes} | \"\n",
    "                  f\"Wins {sum(win_history)}/{len(win_history)} | Win rate {win_rate:.3f} | Time {dt}\")\n",
    "\n",
    "        # early stopping if agent perfect\n",
    "        if win_rate > 0.9:\n",
    "            epsilon = 0.05\n",
    "            if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "                print(f\"Reached 100% win rate at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    # total training time\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" %\n",
    "          (epoch, max_memory, data_size, format_time(seconds)))\n",
    "    return seconds\n",
    "\n",
    "\n",
    "# Utility for pretty time display\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        return \"%.1f seconds\" % seconds\n",
    "    elif seconds < 4000:\n",
    "        return \"%.2f minutes\" % (seconds / 60.0)\n",
    "    else:\n",
    "        return \"%.2f hours\" % (seconds / 3600.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Model\n",
    "\n",
    "Now we will start testing the deep Q-learning implementation. To begin, select **Cell**, then **Run All** from the menu bar. This will run your notebook. As it runs, you should see output begin to appear beneath the next few cells. The code below creates an instance of TreasureMaze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f97d72948>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = TreasureMaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you will build your model and train it using deep Q-learning. Note: This step takes several minutes to fully run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000/15000 | Loss 0.0025 | Episodes 151 | Wins 0/1 | Win rate 0.000 | Time 0:00:10.930180\n",
      "Epoch 00050/15000 | Loss 0.0027 | Episodes 156 | Wins 29/51 | Win rate 0.656 | Time 0:06:23.449361\n",
      "Epoch 00100/15000 | Loss 0.0027 | Episodes 151 | Wins 49/101 | Win rate 0.344 | Time 0:13:18.643970\n",
      "Epoch 00150/15000 | Loss 0.0059 | Episodes 134 | Wins 77/151 | Win rate 0.562 | Time 0:18:58.903329\n",
      "Epoch 00200/15000 | Loss 0.0019 | Episodes 4 | Wins 104/201 | Win rate 0.500 | Time 0:24:54.119875\n",
      "Epoch 00250/15000 | Loss 0.0072 | Episodes 8 | Wins 138/251 | Win rate 0.688 | Time 0:30:06.047650\n",
      "Epoch 00300/15000 | Loss 0.0035 | Episodes 109 | Wins 168/301 | Win rate 0.719 | Time 0:35:14.612468\n",
      "Epoch 00350/15000 | Loss 0.0092 | Episodes 15 | Wins 201/351 | Win rate 0.562 | Time 0:39:44.464465\n",
      "Epoch 00400/15000 | Loss 0.0035 | Episodes 19 | Wins 228/401 | Win rate 0.562 | Time 0:45:13.097545\n",
      "Epoch 00450/15000 | Loss 0.0038 | Episodes 8 | Wins 267/451 | Win rate 0.719 | Time 0:49:35.259957\n",
      "Epoch 00500/15000 | Loss 0.0035 | Episodes 19 | Wins 294/501 | Win rate 0.594 | Time 0:55:19.942255\n",
      "Epoch 00550/15000 | Loss 0.0053 | Episodes 4 | Wins 329/551 | Win rate 0.750 | Time 0:59:58.869772\n",
      "Epoch 00600/15000 | Loss 0.0029 | Episodes 27 | Wins 360/601 | Win rate 0.656 | Time 1:04:58.356587\n",
      "Epoch 00650/15000 | Loss 0.0036 | Episodes 22 | Wins 389/651 | Win rate 0.562 | Time 1:10:03.627136\n",
      "Epoch 00700/15000 | Loss 0.0014 | Episodes 134 | Wins 425/701 | Win rate 0.750 | Time 1:13:49.239706\n",
      "Epoch 00750/15000 | Loss 0.0011 | Episodes 131 | Wins 467/751 | Win rate 0.938 | Time 1:16:29.284283\n",
      "Epoch 00800/15000 | Loss 0.0045 | Episodes 21 | Wins 507/801 | Win rate 0.875 | Time 1:19:57.312363\n",
      "Epoch 00850/15000 | Loss 0.0052 | Episodes 33 | Wins 547/851 | Win rate 0.750 | Time 1:23:17.173289\n",
      "Epoch 00900/15000 | Loss 0.0053 | Episodes 31 | Wins 585/901 | Win rate 0.781 | Time 1:26:46.984468\n",
      "Epoch 00950/15000 | Loss 0.0176 | Episodes 138 | Wins 630/951 | Win rate 0.969 | Time 1:29:32.291803\n",
      "Epoch 01000/15000 | Loss 0.0003 | Episodes 21 | Wins 678/1001 | Win rate 1.000 | Time 1:31:16.961113\n",
      "Epoch 01050/15000 | Loss 0.0030 | Episodes 30 | Wins 726/1051 | Win rate 0.938 | Time 1:33:35.963840\n",
      "Epoch 01100/15000 | Loss 0.0018 | Episodes 2 | Wins 774/1101 | Win rate 1.000 | Time 1:35:55.795721\n",
      "Epoch 01150/15000 | Loss 0.0013 | Episodes 9 | Wins 824/1151 | Win rate 1.000 | Time 1:37:21.627369\n",
      "Epoch 01200/15000 | Loss 0.0009 | Episodes 4 | Wins 874/1201 | Win rate 1.000 | Time 1:38:27.230694\n",
      "Epoch 01250/15000 | Loss 0.0017 | Episodes 6 | Wins 924/1251 | Win rate 1.000 | Time 1:39:58.942395\n",
      "Epoch 01300/15000 | Loss 0.0011 | Episodes 38 | Wins 973/1301 | Win rate 1.000 | Time 1:41:33.053190\n",
      "Epoch 01350/15000 | Loss 0.0020 | Episodes 9 | Wins 1023/1351 | Win rate 1.000 | Time 1:43:00.769708\n",
      "Epoch 01400/15000 | Loss 0.0001 | Episodes 51 | Wins 1073/1401 | Win rate 1.000 | Time 1:44:24.015275\n",
      "Epoch 01450/15000 | Loss 0.0003 | Episodes 26 | Wins 1123/1451 | Win rate 1.000 | Time 1:45:41.665118\n",
      "Epoch 01500/15000 | Loss 0.0014 | Episodes 1 | Wins 1173/1501 | Win rate 1.000 | Time 1:47:25.935943\n",
      "Epoch 01550/15000 | Loss 0.0003 | Episodes 22 | Wins 1223/1551 | Win rate 1.000 | Time 1:48:51.209765\n",
      "Epoch 01600/15000 | Loss 0.0006 | Episodes 27 | Wins 1273/1601 | Win rate 1.000 | Time 1:50:06.965264\n",
      "Epoch 01650/15000 | Loss 0.0014 | Episodes 2 | Wins 1323/1651 | Win rate 1.000 | Time 1:51:39.479516\n",
      "Epoch 01700/15000 | Loss 0.0007 | Episodes 25 | Wins 1373/1701 | Win rate 1.000 | Time 1:53:18.862868\n",
      "Epoch 01750/15000 | Loss 0.0008 | Episodes 31 | Wins 1423/1751 | Win rate 1.000 | Time 1:54:47.795618\n",
      "Epoch 01800/15000 | Loss 0.0004 | Episodes 26 | Wins 1473/1801 | Win rate 1.000 | Time 1:56:18.737788\n",
      "Epoch 01850/15000 | Loss 0.0015 | Episodes 21 | Wins 1523/1851 | Win rate 1.000 | Time 1:58:02.857913\n",
      "Epoch 01900/15000 | Loss 0.0011 | Episodes 4 | Wins 1573/1901 | Win rate 1.000 | Time 1:59:28.819282\n",
      "Epoch 01950/15000 | Loss 0.0013 | Episodes 13 | Wins 1623/1951 | Win rate 1.000 | Time 2:00:26.689981\n",
      "Epoch 02000/15000 | Loss 0.0008 | Episodes 26 | Wins 1673/2001 | Win rate 1.000 | Time 2:02:13.073226\n",
      "Epoch 02050/15000 | Loss 0.0002 | Episodes 19 | Wins 1723/2051 | Win rate 1.000 | Time 2:03:36.390300\n",
      "Epoch 02100/15000 | Loss 0.0002 | Episodes 26 | Wins 1773/2101 | Win rate 1.000 | Time 2:04:42.743794\n",
      "Epoch 02150/15000 | Loss 0.0006 | Episodes 45 | Wins 1823/2151 | Win rate 1.000 | Time 2:06:01.631123\n",
      "Epoch 02200/15000 | Loss 0.0004 | Episodes 15 | Wins 1873/2201 | Win rate 1.000 | Time 2:07:52.491369\n",
      "Epoch 02250/15000 | Loss 0.0004 | Episodes 26 | Wins 1923/2251 | Win rate 1.000 | Time 2:08:58.635884\n",
      "Epoch 02300/15000 | Loss 0.0011 | Episodes 30 | Wins 1973/2301 | Win rate 1.000 | Time 2:10:01.087516\n",
      "Epoch 02350/15000 | Loss 0.0011 | Episodes 2 | Wins 2023/2351 | Win rate 1.000 | Time 2:11:05.426893\n",
      "Epoch 02400/15000 | Loss 0.0010 | Episodes 11 | Wins 2073/2401 | Win rate 1.000 | Time 2:12:30.230088\n",
      "Epoch 02450/15000 | Loss 0.0006 | Episodes 5 | Wins 2123/2451 | Win rate 1.000 | Time 2:13:44.440807\n",
      "Epoch 02500/15000 | Loss 0.0011 | Episodes 22 | Wins 2173/2501 | Win rate 1.000 | Time 2:15:02.682759\n",
      "Epoch 02550/15000 | Loss 0.0002 | Episodes 26 | Wins 2223/2551 | Win rate 1.000 | Time 2:16:28.236224\n",
      "Epoch 02600/15000 | Loss 0.0011 | Episodes 27 | Wins 2273/2601 | Win rate 1.000 | Time 2:17:50.817440\n",
      "Epoch 02650/15000 | Loss 0.0003 | Episodes 25 | Wins 2323/2651 | Win rate 1.000 | Time 2:19:03.020820\n",
      "Epoch 02700/15000 | Loss 0.0009 | Episodes 25 | Wins 2373/2701 | Win rate 1.000 | Time 2:20:15.502892\n",
      "Epoch 02750/15000 | Loss 0.0004 | Episodes 17 | Wins 2423/2751 | Win rate 1.000 | Time 2:21:33.882252\n",
      "Epoch 02800/15000 | Loss 0.0002 | Episodes 30 | Wins 2473/2801 | Win rate 1.000 | Time 2:22:55.911745\n",
      "Epoch 02850/15000 | Loss 0.0011 | Episodes 6 | Wins 2523/2851 | Win rate 1.000 | Time 2:24:07.244164\n",
      "Epoch 02900/15000 | Loss 0.0005 | Episodes 19 | Wins 2573/2901 | Win rate 1.000 | Time 2:25:25.286583\n",
      "Epoch 02950/15000 | Loss 0.0004 | Episodes 18 | Wins 2623/2951 | Win rate 1.000 | Time 2:26:31.953437\n",
      "Epoch 03000/15000 | Loss 0.0001 | Episodes 21 | Wins 2673/3001 | Win rate 1.000 | Time 2:28:15.833662\n",
      "Epoch 03050/15000 | Loss 0.0016 | Episodes 11 | Wins 2723/3051 | Win rate 1.000 | Time 2:29:30.950150\n",
      "Epoch 03100/15000 | Loss 0.0030 | Episodes 28 | Wins 2773/3101 | Win rate 1.000 | Time 2:30:54.941310\n",
      "Epoch 03150/15000 | Loss 0.0013 | Episodes 17 | Wins 2823/3151 | Win rate 1.000 | Time 2:32:26.869658\n",
      "Epoch 03200/15000 | Loss 0.0014 | Episodes 12 | Wins 2873/3201 | Win rate 1.000 | Time 2:33:31.575467\n",
      "Epoch 03250/15000 | Loss 0.0015 | Episodes 17 | Wins 2923/3251 | Win rate 1.000 | Time 2:34:51.967520\n",
      "Epoch 03300/15000 | Loss 0.0032 | Episodes 9 | Wins 2972/3301 | Win rate 0.969 | Time 2:37:17.954200\n",
      "Epoch 03350/15000 | Loss 0.0005 | Episodes 20 | Wins 3022/3351 | Win rate 1.000 | Time 2:38:33.782225\n",
      "Epoch 03400/15000 | Loss 0.0018 | Episodes 18 | Wins 3072/3401 | Win rate 1.000 | Time 2:39:50.177677\n",
      "Epoch 03450/15000 | Loss 0.0003 | Episodes 29 | Wins 3122/3451 | Win rate 1.000 | Time 2:41:13.141734\n",
      "Epoch 03500/15000 | Loss 0.0004 | Episodes 9 | Wins 3172/3501 | Win rate 1.000 | Time 2:42:40.959313\n",
      "Epoch 03550/15000 | Loss 0.0007 | Episodes 22 | Wins 3222/3551 | Win rate 1.000 | Time 2:44:06.857340\n",
      "Epoch 03600/15000 | Loss 0.0015 | Episodes 11 | Wins 3272/3601 | Win rate 1.000 | Time 2:45:18.433149\n",
      "Epoch 03650/15000 | Loss 0.0004 | Episodes 4 | Wins 3322/3651 | Win rate 1.000 | Time 2:46:34.291595\n",
      "Epoch 03700/15000 | Loss 0.0003 | Episodes 7 | Wins 3372/3701 | Win rate 1.000 | Time 2:48:06.718827\n",
      "Epoch 03750/15000 | Loss 0.0023 | Episodes 24 | Wins 3422/3751 | Win rate 1.000 | Time 2:49:30.717569\n",
      "Epoch 03800/15000 | Loss 0.0015 | Episodes 21 | Wins 3471/3801 | Win rate 1.000 | Time 2:51:54.928944\n",
      "Epoch 03850/15000 | Loss 0.0005 | Episodes 34 | Wins 3521/3851 | Win rate 1.000 | Time 2:53:24.133339\n",
      "Epoch 03900/15000 | Loss 0.0010 | Episodes 24 | Wins 3571/3901 | Win rate 1.000 | Time 2:54:28.721834\n",
      "Epoch 03950/15000 | Loss 0.0010 | Episodes 22 | Wins 3621/3951 | Win rate 1.000 | Time 2:55:50.085572\n",
      "Epoch 04000/15000 | Loss 0.0003 | Episodes 26 | Wins 3671/4001 | Win rate 1.000 | Time 2:57:29.478161\n",
      "Epoch 04050/15000 | Loss 0.0004 | Episodes 19 | Wins 3721/4051 | Win rate 1.000 | Time 2:58:44.819047\n",
      "Epoch 04100/15000 | Loss 0.0013 | Episodes 33 | Wins 3771/4101 | Win rate 1.000 | Time 3:00:10.824591\n",
      "Epoch 04150/15000 | Loss 0.0118 | Episodes 3 | Wins 3821/4151 | Win rate 1.000 | Time 3:02:03.305419\n",
      "Epoch 04200/15000 | Loss 0.0013 | Episodes 18 | Wins 3870/4201 | Win rate 1.000 | Time 3:04:05.585620\n",
      "Epoch 04250/15000 | Loss 0.0003 | Episodes 7 | Wins 3920/4251 | Win rate 1.000 | Time 3:05:12.333361\n",
      "Epoch 04300/15000 | Loss 0.0016 | Episodes 25 | Wins 3970/4301 | Win rate 1.000 | Time 3:07:13.070680\n",
      "Epoch 04350/15000 | Loss 0.0027 | Episodes 17 | Wins 4020/4351 | Win rate 1.000 | Time 3:08:45.488253\n",
      "Epoch 04400/15000 | Loss 0.0009 | Episodes 9 | Wins 4070/4401 | Win rate 1.000 | Time 3:09:59.320010\n",
      "Epoch 04450/15000 | Loss 0.0008 | Episodes 19 | Wins 4120/4451 | Win rate 1.000 | Time 3:11:15.784918\n",
      "Epoch 04500/15000 | Loss 0.0017 | Episodes 16 | Wins 4165/4501 | Win rate 0.969 | Time 3:14:12.575225\n",
      "Epoch 04550/15000 | Loss 0.0050 | Episodes 21 | Wins 4215/4551 | Win rate 1.000 | Time 3:16:02.522412\n",
      "Epoch 04600/15000 | Loss 0.0022 | Episodes 18 | Wins 4264/4601 | Win rate 1.000 | Time 3:18:34.050875\n",
      "Epoch 04650/15000 | Loss 0.0003 | Episodes 18 | Wins 4314/4651 | Win rate 1.000 | Time 3:19:42.730138\n",
      "Epoch 04700/15000 | Loss 0.0010 | Episodes 27 | Wins 4364/4701 | Win rate 1.000 | Time 3:21:01.945438\n",
      "Epoch 04750/15000 | Loss 0.0002 | Episodes 13 | Wins 4414/4751 | Win rate 1.000 | Time 3:22:13.480360\n",
      "Epoch 04800/15000 | Loss 0.0015 | Episodes 10 | Wins 4464/4801 | Win rate 1.000 | Time 3:23:16.826026\n",
      "Epoch 04850/15000 | Loss 0.0002 | Episodes 22 | Wins 4514/4851 | Win rate 1.000 | Time 3:24:25.434721\n",
      "Epoch 04900/15000 | Loss 0.0065 | Episodes 1 | Wins 4561/4901 | Win rate 0.906 | Time 3:26:31.021944\n",
      "Epoch 04950/15000 | Loss 0.0008 | Episodes 3 | Wins 4611/4951 | Win rate 1.000 | Time 3:27:49.534055\n",
      "Epoch 05000/15000 | Loss 0.0010 | Episodes 30 | Wins 4661/5001 | Win rate 1.000 | Time 3:29:05.641424\n",
      "Epoch 05050/15000 | Loss 0.0030 | Episodes 47 | Wins 4711/5051 | Win rate 1.000 | Time 3:30:24.818882\n",
      "Epoch 05100/15000 | Loss 0.0009 | Episodes 30 | Wins 4761/5101 | Win rate 1.000 | Time 3:31:47.113625\n",
      "Epoch 05150/15000 | Loss 1341.9781 | Episodes 41 | Wins 4808/5151 | Win rate 0.906 | Time 3:33:41.407284\n",
      "Epoch 05200/15000 | Loss 0.1100 | Episodes 134 | Wins 4811/5201 | Win rate 0.062 | Time 3:42:36.347016\n",
      "Epoch 05250/15000 | Loss 0.2395 | Episodes 141 | Wins 4816/5251 | Win rate 0.094 | Time 3:50:53.426781\n",
      "Epoch 05300/15000 | Loss 165025.3281 | Episodes 134 | Wins 4829/5301 | Win rate 0.312 | Time 3:59:30.504770\n",
      "Epoch 05350/15000 | Loss 28998.9609 | Episodes 134 | Wins 4839/5351 | Win rate 0.219 | Time 4:07:56.322213\n",
      "Epoch 05400/15000 | Loss 1153.2487 | Episodes 135 | Wins 4845/5401 | Win rate 0.156 | Time 4:16:10.725757\n",
      "Epoch 05450/15000 | Loss 34404.3594 | Episodes 139 | Wins 4849/5451 | Win rate 0.094 | Time 4:24:48.805319\n",
      "Epoch 05500/15000 | Loss 90586.6406 | Episodes 136 | Wins 4852/5501 | Win rate 0.031 | Time 4:33:23.215910\n",
      "Epoch 05550/15000 | Loss 28518386.0000 | Episodes 131 | Wins 4859/5551 | Win rate 0.188 | Time 4:41:25.777900\n",
      "Epoch 05600/15000 | Loss 299592.6562 | Episodes 133 | Wins 4861/5601 | Win rate 0.000 | Time 4:49:57.003362\n",
      "Epoch 05650/15000 | Loss 413552.4375 | Episodes 136 | Wins 4869/5651 | Win rate 0.062 | Time 4:57:36.035783\n"
     ]
    }
   ],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will check to see if the model passes the completion check. Note: This could take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_check(model, qmaze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will test your model for one game. It will start the pirate at the top-left corner and run play_game. The agent should find a path from the starting position to the target (treasure). The treasure is located in the bottom-right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_start = (0, 0)\n",
    "play_game(model, qmaze, pirate_start)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Submit Your Work\n",
    "After you have finished creating the code for your notebook, save your work. Make sure that your notebook contains your name in the filename (e.g. Doe_Jane_ProjectTwo.ipynb). This will help your instructor access and grade your work easily. Download a copy of your IPYNB file and submit it to Brightspace. Refer to the Jupyter Notebook in Apporto Tutorial if you need help with these tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
